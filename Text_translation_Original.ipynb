{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text translation-Original",
      "provenance": [],
      "collapsed_sections": [
        "DAA3MzbyoAS5"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mwadz/Text_tanslation/blob/main/Text_translation_Original.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Translation Using Neural Networks"
      ],
      "metadata": {
        "id": "lVIogMSUpCEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Defining the Question"
      ],
      "metadata": {
        "id": "3ZF_Do3ln3nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i) Specifying the Question"
      ],
      "metadata": {
        "id": "DAA3MzbyoAS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use neural networks to translate English text to a local Kenyan language(Kalenjin)."
      ],
      "metadata": {
        "id": "QgtAD41NoGS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ii) Defining the metrics of success"
      ],
      "metadata": {
        "id": "c5_pT03EoGyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a model that can accurately translate English text to Kalenjin with an accuracy score of at least 85%"
      ],
      "metadata": {
        "id": "zFqK4GE1oOAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iii) Understanding the context"
      ],
      "metadata": {
        "id": "d34eGBTqoOxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several translation websites that mostly translate between international languages such as English to Swahili. In Kenya, there a professional bodies that offer translation and interpretation services. Hiring these services can be quite expensive especially when trying to communicate an important information such as constitution interpretation to a pre-dominantly native speaking community. Having a web application can greatly reduce this burden of having to outsource translation services everytime they are needed. \n"
      ],
      "metadata": {
        "id": "1--RTCl7oWD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iv) Recording the Experimental Design"
      ],
      "metadata": {
        "id": "YnBZZo2goaDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Loading the datasets.\n",
        "\n",
        "2. Cleaning the datasets.\n",
        "\n",
        "3. Preprocessing.\n",
        "\n",
        "4. Creating a TensorFlow model.\n",
        "\n",
        "5. Test Processing.\n",
        "\n",
        "6. Training the model.\n",
        "\n",
        "7. Translating.\n",
        "\n",
        "8. Visualizing the process."
      ],
      "metadata": {
        "id": "u9tk83svoium"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### v) Relevance of the data"
      ],
      "metadata": {
        "id": "KUWkCsvEojSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data used in this project is for performing Text Translation using Neural Networks. The dataset link : https://drive.google.com/drive/folders/1qJgQvNd99E_U6oitRIToOdXPbjqEHqnG?usp=sharing"
      ],
      "metadata": {
        "id": "CQ6TBbY3tKNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Installations"
      ],
      "metadata": {
        "id": "gfHEgJdgwdVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"tensorflow-text==2.8.*\""
      ],
      "metadata": {
        "id": "DUQSvTzIwcoe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc3c0b04-1bab-4f7e-a227-6281b99453d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-text==2.8.* in /usr/local/lib/python3.7/dist-packages (2.8.2)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (2.8.0+zzzcolab20220506162203)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (0.12.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.21.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.25.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.46.1)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (14.0.1)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.1.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.6.3)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.5.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Importing the libraries"
      ],
      "metadata": {
        "id": "nV1hwDh9s1ZP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7PCjt709-91",
        "outputId": "43602634-4411-4b5b-d975-ee0da87c880f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "# Import TensorFlow >= 1.10 and enable eager execution\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "print(tf.__version__)#to check the tensorflow version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Loading the datasets"
      ],
      "metadata": {
        "id": "_7oYNhBitA8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the datasets\n",
        "english = pd.read_csv('/content/english.txt', sep='delimiter', engine = 'python', header=None)\n",
        "\n",
        "kale = pd.read_csv('/content/kale.txt', sep='delimiter',  engine = 'python', header=None)\n"
      ],
      "metadata": {
        "id": "C6rq-9h7-LVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Previewing the datasets"
      ],
      "metadata": {
        "id": "OXmwhiAZtO7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the shape of the various datasets\n",
        "files = [english, kale]\n",
        "dataset_names = ['English','Kalenjin']\n",
        "for file in files:\n",
        "  #for index in range(len(dataset_names)):\n",
        "    rows, columns = file.shape\n",
        "    print(f'The dataset has {rows} rows and {columns} columns')\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6LnmJjnt3PE",
        "outputId": "113b70ec-39dc-4308-cfca-22d28e13576b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset has 176 rows and 1 columns\n",
            "The dataset has 176 rows and 1 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Pre_processing"
      ],
      "metadata": {
        "id": "ItSP4diOv_-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing steps includes\n",
        "\n",
        "- Converting the unicode file to ascii\n",
        "- Creating a space between a word and the punctuation following it\n",
        "eg: “he is a boy.” => “he is a boy .” Reference\n",
        "- Replacing everything with space except (a-z, A-Z, “.”, “?”, “!”, “,”)\n",
        "- Adding a start and an end token to the sentence so that the model know when to start and stop predicting.\n",
        "- Removing the accents\n",
        "- Cleaning the sentences\n",
        "- Return word pairs in the format: [ENGLISH, KALENJIN]\n",
        "- Creating a word -> index mapping (e.g,. 'Further' -> 5) and vice-versa. (e.g., 5 -> 'Further' ) for each language."
      ],
      "metadata": {
        "id": "wKESEW4qxFGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an index column for Kalenjin file\n",
        "kale['index_col'] = kale.index"
      ],
      "metadata": {
        "id": "9Ns6OegFPV-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating an index column for Kalenjin file\n",
        "english['index_col'] = kale.index"
      ],
      "metadata": {
        "id": "-LYMqkFVw8Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Joining the English and Kalenjin file with the Index column\n",
        "df_kale = pd.merge(english, kale, on = 'index_col')"
      ],
      "metadata": {
        "id": "vvDFhp9CObTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming the Kalenjin Columns\n",
        "df_kale.head()\n",
        "df_kale.columns = ['feature', 'index', 'target']"
      ],
      "metadata": {
        "id": "LnP8rk38PtsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the Index column in the Kalenjjin file\n",
        "df_kale.columns\n",
        "df_kale = df_kale.drop(columns = ['index'])"
      ],
      "metadata": {
        "id": "UhVBZIzUQow5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the first rows on the Kalenjin file\n",
        "df_kale.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CIEp-CbeQypJ",
        "outputId": "c2cc2f4e-5e48-4d09-f181-7e2423b6382b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             feature  \\\n",
              "0  Blessed are the undefiled in the way, who walk...   \n",
              "1  2 Blessed are they that keep his testimonies, ...   \n",
              "2  3 They also do no iniquity: they walk in his w...   \n",
              "3  4 Thou hast commanded us to keep thy precepts ...   \n",
              "4  5 O that my ways were directed to keep thy sta...   \n",
              "\n",
              "                                              target  \n",
              "0  Boiboen che igesunotgei eng’ oret, Che bendote...  \n",
              "1  Boiboen ichek che ribei baornatosiekyik, Che c...  \n",
              "2  Ee, mayaei ichek che ma bo iman; Bendote ortin...  \n",
              "3  Kiing’at konetisiosieguk, Ile kisub eng’ kagii...  \n",
              "4  Ee, kata mie nda ka kimen ortinwekyuk Si kobii...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00391f43-8afc-45cb-90b3-7448e11b9011\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Blessed are the undefiled in the way, who walk...</td>\n",
              "      <td>Boiboen che igesunotgei eng’ oret, Che bendote...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2 Blessed are they that keep his testimonies, ...</td>\n",
              "      <td>Boiboen ichek che ribei baornatosiekyik, Che c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3 They also do no iniquity: they walk in his w...</td>\n",
              "      <td>Ee, mayaei ichek che ma bo iman; Bendote ortin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4 Thou hast commanded us to keep thy precepts ...</td>\n",
              "      <td>Kiing’at konetisiosieguk, Ile kisub eng’ kagii...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5 O that my ways were directed to keep thy sta...</td>\n",
              "      <td>Ee, kata mie nda ka kimen ortinwekyuk Si kobii...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00391f43-8afc-45cb-90b3-7448e11b9011')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-00391f43-8afc-45cb-90b3-7448e11b9011 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-00391f43-8afc-45cb-90b3-7448e11b9011');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the numbers at the beginning of the feature column\n",
        "df_kale['feature'] = df_kale['feature'].str.replace('\\d+', '')\n",
        "\n",
        "df_kale.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "pi7fEmJ4RKrg",
        "outputId": "fc0ea80c-8eb2-4e7c-c3b2-23ad2aebf2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             feature  \\\n",
              "0  Blessed are the undefiled in the way, who walk...   \n",
              "1   Blessed are they that keep his testimonies, a...   \n",
              "2   They also do no iniquity: they walk in his ways.   \n",
              "3   Thou hast commanded us to keep thy precepts d...   \n",
              "4   O that my ways were directed to keep thy stat...   \n",
              "\n",
              "                                              target  \n",
              "0  Boiboen che igesunotgei eng’ oret, Che bendote...  \n",
              "1  Boiboen ichek che ribei baornatosiekyik, Che c...  \n",
              "2  Ee, mayaei ichek che ma bo iman; Bendote ortin...  \n",
              "3  Kiing’at konetisiosieguk, Ile kisub eng’ kagii...  \n",
              "4  Ee, kata mie nda ka kimen ortinwekyuk Si kobii...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-403edd40-9a43-462c-964a-01c44786d167\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Blessed are the undefiled in the way, who walk...</td>\n",
              "      <td>Boiboen che igesunotgei eng’ oret, Che bendote...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Blessed are they that keep his testimonies, a...</td>\n",
              "      <td>Boiboen ichek che ribei baornatosiekyik, Che c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>They also do no iniquity: they walk in his ways.</td>\n",
              "      <td>Ee, mayaei ichek che ma bo iman; Bendote ortin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Thou hast commanded us to keep thy precepts d...</td>\n",
              "      <td>Kiing’at konetisiosieguk, Ile kisub eng’ kagii...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>O that my ways were directed to keep thy stat...</td>\n",
              "      <td>Ee, kata mie nda ka kimen ortinwekyuk Si kobii...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-403edd40-9a43-462c-964a-01c44786d167')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-403edd40-9a43-462c-964a-01c44786d167 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-403edd40-9a43-462c-964a-01c44786d167');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_kale['feature'] = df_kale['feature'].str.replace('\\d+', '')\n",
        "\n",
        "df_kale['target'] = df_kale['target'].str.replace('\\d+', '')\n",
        "\n",
        "df_kale"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "JxzDPMwJc3yR",
        "outputId": "3b2a2d18-31db-46a6-8d6e-991a985eca38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               feature  \\\n",
              "0    Blessed are the undefiled in the way, who walk...   \n",
              "1     Blessed are they that keep his testimonies, a...   \n",
              "2     They also do no iniquity: they walk in his ways.   \n",
              "3     Thou hast commanded us to keep thy precepts d...   \n",
              "4     O that my ways were directed to keep thy stat...   \n",
              "..                                                 ...   \n",
              "171   My tongue shall speak of thy word: for all th...   \n",
              "172   Let thine hand help me; for I have chosen thy...   \n",
              "173   I have longed for thy salvation, O Lord; and ...   \n",
              "174   Let my soul live, and it shall praise thee; a...   \n",
              "175   I have gone astray like a lost sheep; seek th...   \n",
              "\n",
              "                                                target  \n",
              "0    Boiboen che igesunotgei eng’ oret, Che bendote...  \n",
              "1    Boiboen ichek che ribei baornatosiekyik, Che c...  \n",
              "2    Ee, mayaei ichek che ma bo iman; Bendote ortin...  \n",
              "3    Kiing’at konetisiosieguk, Ile kisub eng’ kagii...  \n",
              "4    Ee, kata mie nda ka kimen ortinwekyuk Si kobii...  \n",
              "..                                                 ...  \n",
              "171  Ingotien ng’elyeptanyu agobo ng’olyondeng’ung’...  \n",
              "172  Ingochobok eung’ung’ kotoreta; Amu kialewen ko...  \n",
              "173  Kigoama emosto agobo yetuneng’ung’, ee Jehovah...  \n",
              "174  Ingosob sobondanyu, si kolosun; Ak ingotoreta ...  \n",
              "175  Kiabetote ko u kechiriet ne betot; cheng’ kibo...  \n",
              "\n",
              "[176 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a4c4ff9a-bf41-466b-ab2f-0fdf8e4162d2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Blessed are the undefiled in the way, who walk...</td>\n",
              "      <td>Boiboen che igesunotgei eng’ oret, Che bendote...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Blessed are they that keep his testimonies, a...</td>\n",
              "      <td>Boiboen ichek che ribei baornatosiekyik, Che c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>They also do no iniquity: they walk in his ways.</td>\n",
              "      <td>Ee, mayaei ichek che ma bo iman; Bendote ortin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Thou hast commanded us to keep thy precepts d...</td>\n",
              "      <td>Kiing’at konetisiosieguk, Ile kisub eng’ kagii...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>O that my ways were directed to keep thy stat...</td>\n",
              "      <td>Ee, kata mie nda ka kimen ortinwekyuk Si kobii...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>My tongue shall speak of thy word: for all th...</td>\n",
              "      <td>Ingotien ng’elyeptanyu agobo ng’olyondeng’ung’...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>Let thine hand help me; for I have chosen thy...</td>\n",
              "      <td>Ingochobok eung’ung’ kotoreta; Amu kialewen ko...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>I have longed for thy salvation, O Lord; and ...</td>\n",
              "      <td>Kigoama emosto agobo yetuneng’ung’, ee Jehovah...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>Let my soul live, and it shall praise thee; a...</td>\n",
              "      <td>Ingosob sobondanyu, si kolosun; Ak ingotoreta ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>I have gone astray like a lost sheep; seek th...</td>\n",
              "      <td>Kiabetote ko u kechiriet ne betot; cheng’ kibo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>176 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4c4ff9a-bf41-466b-ab2f-0fdf8e4162d2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a4c4ff9a-bf41-466b-ab2f-0fdf8e4162d2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a4c4ff9a-bf41-466b-ab2f-0fdf8e4162d2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = df_kale['target'].to_list()"
      ],
      "metadata": {
        "id": "npB5UQG4epUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targ = df_kale['feature'].to_list()"
      ],
      "metadata": {
        "id": "5oa6ivQBcE4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Creating tf_dataset"
      ],
      "metadata": {
        "id": "tPz-5B-ve5OG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a tf.data.Dataset of strings that shuffles and batches them efficiently:"
      ],
      "metadata": {
        "id": "skpuhugPjU6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tells TensorFlow to create a buffer of at most buffer_size elements, and a background thread to fill that buffer in the background\n",
        "BUFFER_SIZE = len(inp)\n",
        "\n",
        "# Number of samples to be feed into the neural network\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Creating the dataset and shuffling it \n",
        "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset "
      ],
      "metadata": {
        "id": "Yn3o0fzUez0p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b17bac26-b900-4cfb-a834-d6c20577053d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for example_input_batch, example_target_batch in dataset.take(1):\n",
        "  print(example_input_batch[:5])\n",
        "  print()\n",
        "  print(example_target_batch[:5])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX98Wa1bHlsE",
        "outputId": "4b85cba0-5260-4bd0-b194-3ba22a839f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'Tonoksei sobondanyu amu emosto Ne tinyei kotugul eng\\xe2\\x80\\x99 kiruogutiguk.'\n",
            " b'Kiing\\xe2\\x80\\x99at baornatosieguk eng\\xe2\\x80\\x99 imanda Ak eng\\xe2\\x80\\x99 lititindo ne o.'\n",
            " b'Mie eng\\xe2\\x80\\x99 ane amu kiginyalila; Si kobiit anetge ng\\xe2\\x80\\x99atutiguk.'\n",
            " b'Ing\\xe2\\x80\\x99omita ng\\xe2\\x80\\x99atutiguk asiir bunikyuk: Amu mi kobota kotugul.'\n",
            " b'Kigoonata buch bounik; Ago iywei muguleldanyu ng\\xe2\\x80\\x99aleguk.'], shape=(5,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b' My soul breaketh for the longing that it hath unto thy judgments at all times.'\n",
            " b' Thy testimonies that thou hast commanded are righteous and very faithful.'\n",
            " b' It is good for me that I have been afflicted; that I might learn thy statutes.'\n",
            " b' Thou through thy commandments hast made me wiser than mine enemies: for they are ever with me.'\n",
            " b' Princes have persecuted me without a cause: but my heart standeth in awe of thy word.'], shape=(5,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Text processing"
      ],
      "metadata": {
        "id": "_kCgCCkofZql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i) Standardization"
      ],
      "metadata": {
        "id": "QxJjvOzYfeSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the model is dealing with multilingual text with a limited vocabulary standardization of the text is crucial. Steps;\n",
        "1.  Unicode normalization to split accented characters\n",
        "2.  replace compatibility characters with their ASCII equivalents."
      ],
      "metadata": {
        "id": "zbsccb0GlpPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text as tf_text"
      ],
      "metadata": {
        "id": "qimZmadJge4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of a text normalized and uni encoded\n",
        "sample_text = tf.constant('Kiacheng’in eng’ muguleldanyu tugul')\n",
        "\n",
        "print(sample_text.numpy())\n",
        "print(tf_text.normalize_utf8(sample_text, 'NFKD').numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaXEDlsSfcDu",
        "outputId": "f9471ad0-0d31-4099-c80f-9be5ebfc2701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'Kiacheng\\xe2\\x80\\x99in eng\\xe2\\x80\\x99 muguleldanyu tugul'\n",
            "b'Kiacheng\\xe2\\x80\\x99in eng\\xe2\\x80\\x99 muguleldanyu tugul'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unicode normalization \n",
        "def tf_lower_and_split_punct(text):\n",
        "  # Split accecented characters.\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ],
      "metadata": {
        "id": "hvbI7A7hgaBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Priniting an example of the original text\n",
        "print(sample_text.numpy().decode())\n",
        "\n",
        "# printing the text afterunicode normalization\n",
        "print(tf_lower_and_split_punct(sample_text).numpy().decode())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTC9ec0vg2w9",
        "outputId": "474d8794-936c-4543-fb06-8886735fbf1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kiacheng’in eng’ muguleldanyu tugul\n",
            "[START] kiachengin eng muguleldanyu tugul [END]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting and coverting input text to sequences of tokens\n",
        "# max_vocab_size limit RAM usage during the initial scan of the training corpus to discover the vocabulary.\n",
        "max_vocab_size = 25000 \n",
        "\n",
        "input_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)"
      ],
      "metadata": {
        "id": "pcG-ObPshAmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading one epoch of the training data with the adapt method \n",
        "input_text_processor.adapt(inp)\n",
        "\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "input_text_processor.get_vocabulary()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euDjRG9ZhEPH",
        "outputId": "ddb74046-6fed-4339-adbd-652633809802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', ',', 'ak', 'eng', 'amu', 'ngatutiguk']"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the Kalenjin TextVectorization layer to build the English layer with .adapt() method\n",
        "output_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor.adapt(targ)\n",
        "output_text_processor.get_vocabulary()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdrMYNIzhKO1",
        "outputId": "056b6200-7b24-405b-aca8-2fc548553130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'thy', '[START]', '[END]', '.', 'i', ',', 'me', 'and']"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the layers created to convert a batch of strings into a batch of token IDs\n",
        "example_tokens = input_text_processor(example_input_batch)\n",
        "example_tokens[:3, :10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGz0sQG2hRI9",
        "outputId": "7dc54cd5-c9e9-4478-8c69-03aa0d645040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
              "array([[  2, 195,  38,   8,  99,  10, 118,  58,   7,  24],\n",
              "       [  2, 142,  22,   7,  60,   6,   7, 261,  10,  87],\n",
              "       [  2,  43,   7,  63,   8, 336,  14,  29, 178,   9]])>"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the token IDs that are zero-padded that can be turned into a mask\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(example_tokens)\n",
        "plt.title('Token IDs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "AMFb-oOlhSBZ",
        "outputId": "1fe3d83d-2b86-49bf-e5fe-b6fe14a49f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "metadata": {},
          "execution_count": 115
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining constants for the model\n",
        "# Embedding layer enables us to convert each word into a fixed length vector of defined size\n",
        "embedding_dim = 512\n",
        "units = 1024"
      ],
      "metadata": {
        "id": "W34FhUHQhXYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  The encoder"
      ],
      "metadata": {
        "id": "yfs8q1paheh8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing to do is build the encoder. The process is as follows:\n",
        "\n",
        "1. Taking a list of token IDs. \n",
        "\n",
        "2. Using the embedding vector for each token.\n",
        "\n",
        "3. Processessing the embeddings into a new sequence "
      ],
      "metadata": {
        "id": "MfGvULMIxdyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the  list of token IDs\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # The GRU RNN layer processes those vectors sequentially.\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(tokens, ('batch', 's'))\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding for each token.\n",
        "    vectors = self.embedding(tokens)\n",
        "    shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
        "\n",
        "    # 3. The GRU processes the embedding sequence.\n",
        "    #    output shape: (batch, s, enc_units)\n",
        "    #    state shape: (batch, enc_units)\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "    shape_checker(output, ('batch', 's', 'enc_units'))\n",
        "    shape_checker(state, ('batch', 'enc_units'))\n",
        "\n",
        "    # 4. Returns the new sequence and its state.\n",
        "    return output, state"
      ],
      "metadata": {
        "id": "JF5tj3-LhaoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the input text to tokens.\n",
        "example_tokens = input_text_processor(example_input_batch)\n",
        "\n",
        "# Encode the input sequence.\n",
        "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)\n",
        "example_enc_output, example_enc_state = encoder(example_tokens)\n",
        "\n",
        "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
        "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
        "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCGAOYOKhhyU",
        "outputId": "3d5c854e-fa2f-412e-a115-bcf03aa22276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch, shape (batch): (16,)\n",
            "Input batch tokens, shape (batch, s): (16, 15)\n",
            "Encoder output, shape (batch, s, units): (16, 15, 1024)\n",
            "Encoder state, shape (batch, units): (16, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  The attention head"
      ],
      "metadata": {
        "id": "VS6yGwmvh7wE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder uses attention to selectively focus on parts of the input sequence. The attention takes a sequence of vectors as input for each example and returns an \"attention\" vector for each example. "
      ],
      "metadata": {
        "id": "pyENzTlKzQJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shapechecker"
      ],
      "metadata": {
        "id": "kY6rStF8huvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to prevent loading of data of wrong shape"
      ],
      "metadata": {
        "id": "tTMhMqrChhVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ShapeChecker():\n",
        "  def __init__(self):\n",
        "    # Keep a cache of every axis-name seen\n",
        "    self.shapes = {}\n",
        "\n",
        "  def __call__(self, tensor, names, broadcast=False):\n",
        "    if not tf.executing_eagerly():\n",
        "      return\n",
        "\n",
        "    if isinstance(names, str):\n",
        "      names = (names,)\n",
        "\n",
        "    shape = tf.shape(tensor)\n",
        "    rank = tf.rank(tensor)\n",
        "\n",
        "    if rank != len(names):\n",
        "      raise ValueError(f'Rank mismatch:\\n'\n",
        "                       f'    found {rank}: {shape.numpy()}\\n'\n",
        "                       f'    expected {len(names)}: {names}\\n')\n",
        "\n",
        "    for i, name in enumerate(names):\n",
        "      if isinstance(name, int):\n",
        "        old_dim = name\n",
        "      else:\n",
        "        old_dim = self.shapes.get(name, None)\n",
        "      new_dim = shape[i]\n",
        "\n",
        "      if (broadcast and new_dim == 1):\n",
        "        continue\n",
        "\n",
        "      if old_dim is None:\n",
        "        # If the axis name is new, add its length to the cache.\n",
        "        self.shapes[name] = new_dim\n",
        "        continue\n",
        "\n",
        "      if new_dim != old_dim:\n",
        "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
        "                         f\"    found: {new_dim}\\n\"\n",
        "                         f\"    expected: {old_dim}\\n\")"
      ],
      "metadata": {
        "id": "RCF51Lophw-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The BahdanauAttention class handles the weight matrices in a pair of dense layers and calls the builtin implementation\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # For Eqn. (4), the  Bahdanau attention\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(query, ('batch', 't', 'query_units'))\n",
        "    shape_checker(value, ('batch', 's', 'value_units'))\n",
        "    shape_checker(mask, ('batch', 's'))\n",
        "\n",
        "    # From Eqn. (4), `W1@ht`.\n",
        "    w1_query = self.W1(query)\n",
        "    shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
        "\n",
        "    # From Eqn. (4), `W2@hs`.\n",
        "    w2_key = self.W2(value)\n",
        "    shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
        "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "CNl8GtARh-_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i) Attention head layer"
      ],
      "metadata": {
        "id": "s8lEay96iFaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a BahdanauAttention layer\n",
        "attention_layer = BahdanauAttention(units)"
      ],
      "metadata": {
        "id": "p3D4q8LqiDDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Excluding the padding\n",
        "(example_tokens != 0).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXZOMPBeiLJT",
        "outputId": "e91a05e7-cc38-419f-a39e-d9470929f4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([16, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Later, the decoder will generate this attention query\n",
        "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
        "\n",
        "# Attend to the encoded tokens\n",
        "\n",
        "context_vector, attention_weights = attention_layer(\n",
        "    query=example_attention_query,\n",
        "    value=example_enc_output,\n",
        "    mask=(example_tokens != 0))\n",
        "\n",
        "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
        "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAIY77zQiOLi",
        "outputId": "46c5cf86-1aae-45db-efe9-8cdbf59de988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (batch_size, query_seq_length, units):           (16, 2, 1024)\n",
            "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (16, 2, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attention weights across the sequences at t=0\n",
        "# t is used for slicing, for selecting different parts of the data.\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(attention_weights[:, 0, :])\n",
        "plt.title('Attention weights')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "S3sxTlpbiSbN",
        "outputId": "8694ab6b-4592-476b-d8ba-c0f2562a248d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "metadata": {},
          "execution_count": 124
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the shape of the attention weights\n",
        "attention_weights.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtKEg3H-iWHO",
        "outputId": "7184e693-6925-4038-e958-3bf9cc77ea8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([16, 2, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_slice = attention_weights[0, 0].numpy()\n",
        "attention_slice = attention_slice[attention_slice != 0]"
      ],
      "metadata": {
        "id": "nLIfNuypiZs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ii) Toogle code"
      ],
      "metadata": {
        "id": "5-EMQsamifCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting attention weights\n",
        "plt.suptitle('Attention weights for one sequence')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "a1 = plt.subplot(1, 2, 1)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "# freeze the xlim\n",
        "plt.xlim(plt.xlim())\n",
        "plt.xlabel('Attention weights')\n",
        "\n",
        "a2 = plt.subplot(1, 2, 2)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "plt.xlabel('Attention weights, zoomed')\n",
        "\n",
        "# zoom in\n",
        "top = max(a1.get_ylim())\n",
        "zoom = 0.85*top\n",
        "a2.set_ylim([0.90*top, top])\n",
        "a1.plot(a1.get_xlim(), [zoom, zoom], color='k')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "kC-M1cWiihZl",
        "outputId": "048e71ee-6306-4541-d6b6-8a36cb54599f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fcbc9e50dd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iii) The decoder"
      ],
      "metadata": {
        "id": "8aJ1-TaViluf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decoder generates predictions for the next output token.\n",
        "1. The decoder receives the complete encoder output.\n",
        "\n",
        "2. It uses an RNN to keep track of what it has generated so far.\n",
        "\n",
        "3. It uses its RNN output as the query to the attention over the encoder's output, producing the context vector.\n",
        "\n",
        "4. It combines the RNN output and the context vector to generate the attention vector.\n",
        "\n",
        "5. It generates logit predictions for the next token based on the attention vector."
      ],
      "metadata": {
        "id": "QRld9r7zaAao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder class and its initializer creates all the necessary layers.\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    # The embedding layer convets token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # The RNN keeps track of what's been generated so far.\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    #  Eqn. (3): converting `ct` to `at`\n",
        "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                    use_bias=False)\n",
        "\n",
        "    # This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
      ],
      "metadata": {
        "id": "y2w6cOE_ijbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import typing\n",
        "from typing import Any, Tuple"
      ],
      "metadata": {
        "id": "4BJX5xOzi2Ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the call method for this layer which  takes and returns multiple tensors.\n",
        "# Organizing those into simple container classes.\n",
        "class DecoderInput(typing.NamedTuple):\n",
        "  new_tokens: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ],
      "metadata": {
        "id": "f7aMUkaXisc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the call method\n",
        "def call(self,\n",
        "         inputs: DecoderInput,\n",
        "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(inputs.new_tokens, ('batch', 't'))\n",
        "  shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
        "  shape_checker(inputs.mask, ('batch', 's'))\n",
        "\n",
        "  if state is not None:\n",
        "    shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "  # Lookup the embeddings\n",
        "  vectors = self.embedding(inputs.new_tokens)\n",
        "  shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
        "\n",
        "  # Process one step with the RNN\n",
        "  rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "  shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
        "  shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "  #  Use the RNN output as the query for the attention over the\n",
        "  # encoder output.\n",
        "  context_vector, attention_weights = self.attention(\n",
        "      query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "  shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
        "  shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "  #  Join the context_vector and rnn_output\n",
        "  #     [ct; ht] shape: (batch t, value_units + query_units)\n",
        "  context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "\n",
        "  #  `at = tanh(Wc@[ct; ht])`\n",
        "  attention_vector = self.Wc(context_and_rnn_output)\n",
        "  shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
        "\n",
        "  # Generate logit predictions:\n",
        "  logits = self.fc(attention_vector)\n",
        "  shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
        "\n",
        "  return DecoderOutput(logits, attention_weights), state"
      ],
      "metadata": {
        "id": "hb46LHidjFpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Decoder.call = call"
      ],
      "metadata": {
        "id": "zyzKL0jmjnK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing  of the decoder \n",
        "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)"
      ],
      "metadata": {
        "id": "PAUFYYjFjJoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the target sequence, and collect the \"[START]\" tokens\n",
        "example_output_tokens = output_text_processor(example_target_batch)\n",
        "\n",
        "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
        "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])"
      ],
      "metadata": {
        "id": "QOCS-aMZjMex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the decoder\n",
        "dec_result, dec_state = decoder(\n",
        "    inputs = DecoderInput(new_tokens=first_token,\n",
        "                          enc_output=example_enc_output,\n",
        "                          mask=(example_tokens != 0)),\n",
        "    state = example_enc_state\n",
        ")\n",
        "\n",
        "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n",
        "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8tAW2qgjPX-",
        "outputId": "7df9b4e7-9100-482e-e7c0-bd84f8ec232c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits shape: (batch_size, t, output_vocab_size) (16, 1, 486)\n",
            "state shape: (batch_size, dec_units) (16, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling a token with the logits\n",
        "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)"
      ],
      "metadata": {
        "id": "r0Sr9yCejyGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoding the token as the first word of the output\n",
        "vocab = np.array(output_text_processor.get_vocabulary())\n",
        "first_word = vocab[sampled_token.numpy()]\n",
        "first_word[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_36L8TSj3RB",
        "outputId": "7f5cfa82-6288-403d-a409-ba2743a64bcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['kindness'],\n",
              "       ['rejoicing'],\n",
              "       ['wiser'],\n",
              "       ['many'],\n",
              "       ['established']], dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the same enc_output, mask and sampled tokens as new tokens.\n",
        "\n",
        "dec_result, dec_state = decoder(\n",
        "    DecoderInput(sampled_token,\n",
        "                 example_enc_output,\n",
        "                 mask=(example_tokens != 0)),\n",
        "    state=dec_state)"
      ],
      "metadata": {
        "id": "5-K4tG9kkEDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a second set of logits using the decoder\n",
        "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n",
        "first_word = vocab[sampled_token.numpy()]\n",
        "first_word[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OSaBZEQkLex",
        "outputId": "9b555b4a-bbd8-4267-c010-8ac6974e4951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['have'],\n",
              "       ['speak'],\n",
              "       ['oppressors'],\n",
              "       ['face'],\n",
              "       ['strengthen']], dtype='<U14')"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Training"
      ],
      "metadata": {
        "id": "-ng_jdDekShC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the model we'll follow the following steps:\n",
        "\n",
        "1. A loss function and optimizer to perform the optimization.\n",
        "\n",
        "2. A training step function defining how to update the model for each input/target batch.\n",
        "\n",
        "3. A training loop to drive the training and save checkpoints."
      ],
      "metadata": {
        "id": "L_FDJCl_gkoW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i) Define the loss function"
      ],
      "metadata": {
        "id": "SCjXzI_1k8Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the loss function and optimizer to perform the optimization.\n",
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    shape_checker = ShapeChecker()\n",
        "    shape_checker(y_true, ('batch', 't'))\n",
        "    shape_checker(y_pred, ('batch', 't', 'logits'))\n",
        "\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    shape_checker(loss, ('batch', 't'))\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    shape_checker(mask, ('batch', 't'))\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)"
      ],
      "metadata": {
        "id": "C2IcOwyKj8At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ii) Implementing the training step"
      ],
      "metadata": {
        "id": "CaII-2g3lLmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing a model class, the training process will be implemented as the train_step method \n",
        "class TrainTranslator(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units,\n",
        "               input_text_processor,\n",
        "               output_text_processor, \n",
        "               use_tf_function=True):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "    decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "    self.use_tf_function = use_tf_function\n",
        "    self.shape_checker = ShapeChecker()\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    self.shape_checker = ShapeChecker()\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)"
      ],
      "metadata": {
        "id": "TTsZs60qkcOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting a batch of input_text, target_text from the tf.data.Dataset.\n",
        "def _preprocess(self, input_text, target_text):\n",
        "  self.shape_checker(input_text, ('batch',))\n",
        "  self.shape_checker(target_text, ('batch',))\n",
        "\n",
        "  # Convert the text to token IDs\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  target_tokens = self.output_text_processor(target_text)\n",
        "  self.shape_checker(input_tokens, ('batch', 's'))\n",
        "  self.shape_checker(target_tokens, ('batch', 't'))\n",
        "\n",
        "  # Convert IDs to masks.\n",
        "  input_mask = input_tokens != 0\n",
        "  self.shape_checker(input_mask, ('batch', 's'))\n",
        "\n",
        "  target_mask = target_tokens != 0\n",
        "  self.shape_checker(target_mask, ('batch', 't'))\n",
        "\n",
        "  return input_tokens, input_mask, target_tokens, target_mask"
      ],
      "metadata": {
        "id": "yR_m1uF4kfzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainTranslator._preprocess = _preprocess"
      ],
      "metadata": {
        "id": "s-oqNm4zlZZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the _train_step method\n",
        "def _train_step(self, inputs):\n",
        "  input_text, target_text = inputs  \n",
        "\n",
        "  (input_tokens, input_mask,\n",
        "   target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "  max_target_length = tf.shape(target_tokens)[1]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Encode the input\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "    self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
        "    self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
        "\n",
        "    # Initialize the decoder's state to the encoder's final state.\n",
        "    # This only works if the encoder and decoder have the same number of\n",
        "    # units.\n",
        "    dec_state = enc_state\n",
        "    loss = tf.constant(0.0)\n",
        "\n",
        "    for t in tf.range(max_target_length-1):\n",
        "      # Pass in two tokens from the target sequence:\n",
        "      # 1. The current input to the decoder.\n",
        "      # 2. The target for the decoder's next prediction.\n",
        "      new_tokens = target_tokens[:, t:t+2]\n",
        "      step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                             enc_output, dec_state)\n",
        "      loss = loss + step_loss\n",
        "\n",
        "    # Average the loss over all non padding tokens.\n",
        "    average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "  # Apply an optimization step\n",
        "  variables = self.trainable_variables \n",
        "  gradients = tape.gradient(average_loss, variables)\n",
        "  self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  # Return a dict mapping metric names to current value\n",
        "  return {'batch_loss': average_loss}"
      ],
      "metadata": {
        "id": "fyD9YlGlkhiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainTranslator._train_step = _train_step"
      ],
      "metadata": {
        "id": "j3mVALlPmbLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "  input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "  # Run the decoder one step.\n",
        "  decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                               enc_output=enc_output,\n",
        "                               mask=input_mask)\n",
        "\n",
        "  dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "  self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
        "  self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
        "  self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
        "\n",
        "  # `self.loss` returns the total for non-padded tokens\n",
        "  y = target_token\n",
        "  y_pred = dec_result.logits\n",
        "  step_loss = self.loss(y, y_pred)\n",
        "\n",
        "  return step_loss, dec_state"
      ],
      "metadata": {
        "id": "dOehrED0md0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainTranslator._loop_step = _loop_step"
      ],
      "metadata": {
        "id": "nNBAPnZwmfFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iii) Test the training step"
      ],
      "metadata": {
        "id": "__19iWQPmiwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a TrainTranslator and configuring it for training using the Model.compile method\n",
        "translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "    use_tf_function=False)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ],
      "metadata": {
        "id": "kf7vv-CqmnDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the train_step model\n",
        "np.log(output_text_processor.vocabulary_size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7YpBQGgmpeq",
        "outputId": "331a9644-d150-42fe-a65a-8d52a1d09287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.186208623900494"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying the tf.function-wrapped _tf_train_step, to maximize performance while training\n",
        "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "def _tf_train_step(self, inputs):\n",
        "  return self._train_step(inputs)"
      ],
      "metadata": {
        "id": "65UeaPVZmsKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainTranslator._tf_train_step = _tf_train_step"
      ],
      "metadata": {
        "id": "1pHTE-FLmw4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator.use_tf_function = True"
      ],
      "metadata": {
        "id": "IrDSsPrsmzC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracing the function\n",
        "translator.train_step([example_input_batch, example_target_batch])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_uZLIwbnChM",
        "outputId": "d4e69953-1157-4871-a17f-593ad0f7fc37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.8308077>}"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing out the Batch loss of our model\n",
        "%%time\n",
        "for n in range(10):\n",
        "  print(translator.train_step([example_input_batch, example_target_batch]))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNHqOaK0nDRa",
        "outputId": "dca5fe73-7709-45c9-d890-87bf077d40d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.7471685>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.607351>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.2584624>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.3019753>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.36408>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.746395>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.5495095>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.2778263>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.0590544>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.9737322>}\n",
            "\n",
            "CPU times: user 43.6 s, sys: 1.35 s, total: 45 s\n",
            "Wall time: 43.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting our batch losses\n",
        "losses = []\n",
        "for n in range(100):\n",
        "  print('.', end='')\n",
        "  logs = translator.train_step([example_input_batch, example_target_batch])\n",
        "  losses.append(logs['batch_loss'].numpy())\n",
        "\n",
        "print()\n",
        "plt.plot(losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "KOl01ZMFnOsT",
        "outputId": "20d5e382-5081-4848-c85b-920b8ffc5485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "....................................................................................................\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fcbc8ddeb10>]"
            ]
          },
          "metadata": {},
          "execution_count": 155
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Building another model to train\n",
        "train_translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ],
      "metadata": {
        "id": "LPHSFGQwnSCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iv) Train the model"
      ],
      "metadata": {
        "id": "pY3YQsfan-O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a couple of epochs by applying the callbacks.Callback method\n",
        "# to collect the history of batch losses\n",
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ],
      "metadata": {
        "id": "TNhN4bJHoBSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying the batch loss using 37 epochs \n",
        "train_translator.fit(dataset, epochs=37,\n",
        "                     callbacks=[batch_loss])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgA08TIfoEvf",
        "outputId": "d51c162f-4316-49ca-9c3c-58fa33558942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/37\n",
            "11/11 [==============================] - 37s 3s/step - batch_loss: 5.2131\n",
            "Epoch 2/37\n",
            "11/11 [==============================] - 29s 3s/step - batch_loss: 4.3024\n",
            "Epoch 3/37\n",
            "11/11 [==============================] - 31s 3s/step - batch_loss: 3.9263\n",
            "Epoch 4/37\n",
            "11/11 [==============================] - 32s 3s/step - batch_loss: 3.5583\n",
            "Epoch 5/37\n",
            "11/11 [==============================] - 30s 3s/step - batch_loss: 3.2238\n",
            "Epoch 6/37\n",
            "11/11 [==============================] - 30s 3s/step - batch_loss: 2.8712\n",
            "Epoch 7/37\n",
            "11/11 [==============================] - 30s 3s/step - batch_loss: 2.5132\n",
            "Epoch 8/37\n",
            "11/11 [==============================] - 30s 3s/step - batch_loss: 2.2499\n",
            "Epoch 9/37\n",
            "11/11 [==============================] - 28s 3s/step - batch_loss: 1.9737\n",
            "Epoch 10/37\n",
            "11/11 [==============================] - 28s 3s/step - batch_loss: 1.7249\n",
            "Epoch 11/37\n",
            "11/11 [==============================] - 29s 3s/step - batch_loss: 1.4304\n",
            "Epoch 12/37\n",
            "11/11 [==============================] - 28s 3s/step - batch_loss: 1.1773\n",
            "Epoch 13/37\n",
            "11/11 [==============================] - 29s 3s/step - batch_loss: 0.9478\n",
            "Epoch 14/37\n",
            "11/11 [==============================] - 29s 3s/step - batch_loss: 0.7684\n",
            "Epoch 15/37\n",
            "11/11 [==============================] - 29s 3s/step - batch_loss: 0.5890\n",
            "Epoch 16/37\n",
            "11/11 [==============================] - 29s 3s/step - batch_loss: 0.4571\n",
            "Epoch 17/37\n",
            "11/11 [==============================] - 30s 3s/step - batch_loss: 0.3477\n",
            "Epoch 18/37\n",
            "11/11 [==============================] - 28s 3s/step - batch_loss: 0.2409\n",
            "Epoch 19/37\n",
            "11/11 [==============================] - 27s 3s/step - batch_loss: 0.1675\n",
            "Epoch 20/37\n",
            "11/11 [==============================] - 29s 3s/step - batch_loss: 0.1231\n",
            "Epoch 21/37\n",
            "11/11 [==============================] - 29s 3s/step - batch_loss: 0.0949\n",
            "Epoch 22/37\n",
            "11/11 [==============================] - 27s 2s/step - batch_loss: 0.0648\n",
            "Epoch 23/37\n",
            "11/11 [==============================] - 29s 3s/step - batch_loss: 0.0488\n",
            "Epoch 24/37\n",
            "11/11 [==============================] - 29s 3s/step - batch_loss: 0.0327\n",
            "Epoch 25/37\n",
            "11/11 [==============================] - 30s 3s/step - batch_loss: 0.0225\n",
            "Epoch 26/37\n",
            "11/11 [==============================] - 28s 3s/step - batch_loss: 0.0164\n",
            "Epoch 27/37\n",
            "11/11 [==============================] - 30s 3s/step - batch_loss: 0.0135\n",
            "Epoch 28/37\n",
            "11/11 [==============================] - 29s 3s/step - batch_loss: 0.0115\n",
            "Epoch 29/37\n",
            "11/11 [==============================] - 28s 3s/step - batch_loss: 0.0103\n",
            "Epoch 30/37\n",
            "11/11 [==============================] - 28s 2s/step - batch_loss: 0.0093\n",
            "Epoch 31/37\n",
            "11/11 [==============================] - 29s 3s/step - batch_loss: 0.0085\n",
            "Epoch 32/37\n",
            "11/11 [==============================] - 27s 2s/step - batch_loss: 0.0079\n",
            "Epoch 33/37\n",
            "11/11 [==============================] - 29s 3s/step - batch_loss: 0.0073\n",
            "Epoch 34/37\n",
            "11/11 [==============================] - 28s 3s/step - batch_loss: 0.0067\n",
            "Epoch 35/37\n",
            "11/11 [==============================] - 31s 3s/step - batch_loss: 0.0063\n",
            "Epoch 36/37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the epochs\n",
        "plt.plot(batch_loss.logs)\n",
        "plt.ylim([0, 3])\n",
        "plt.xlabel('Batch #')\n",
        "plt.ylabel('CE/token')"
      ],
      "metadata": {
        "id": "cPsGUSJkqWi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Translate"
      ],
      "metadata": {
        "id": "079uq-8ZqcCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Executing the full text => texttranslation\n",
        "# This is by inverting the text => token IDsmapping provided by the output_text_processor\n",
        "class Translator(tf.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, input_text_processor,\n",
        "               output_text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "    self.output_token_string_from_index = (\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            mask_token='',\n",
        "            invert=True))\n",
        "\n",
        "    # The output should never generate padding, unknown, or start.\n",
        "    index_from_string = tf.keras.layers.StringLookup(\n",
        "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
        "    token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
        "\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask\n",
        "\n",
        "    self.start_token = index_from_string(tf.constant('[START]'))\n",
        "    self.end_token = index_from_string(tf.constant('[END]'))"
      ],
      "metadata": {
        "id": "pee0S4sxqbZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        ")"
      ],
      "metadata": {
        "id": "Q1zOGt-eqjgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i) Convert IDs to text"
      ],
      "metadata": {
        "id": "pDZbxQB3qp3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the tokens_to_text which converts from token IDs to human readable text.\n",
        "def tokens_to_text(self, result_tokens):\n",
        "  shape_checker = ShapeChecker()\n",
        "  shape_checker(result_tokens, ('batch', 't'))\n",
        "  result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "  shape_checker(result_text_tokens, ('batch', 't'))\n",
        "\n",
        "  result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                       axis=1, separator=' ')\n",
        "  shape_checker(result_text, ('batch'))\n",
        "\n",
        "  result_text = tf.strings.strip(result_text)\n",
        "  shape_checker(result_text, ('batch',))\n",
        "  return result_text"
      ],
      "metadata": {
        "id": "KphbGFO7q9Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Translator.tokens_to_text = tokens_to_text"
      ],
      "metadata": {
        "id": "NCmFMywfqsO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputting some random token IDs and see what it generates (example)\n",
        "example_output_tokens = tf.random.uniform(\n",
        "    shape=[5, 2], minval=0, dtype=tf.int64,\n",
        "    maxval=output_text_processor.vocabulary_size())\n",
        "translator.tokens_to_text(example_output_tokens).numpy()"
      ],
      "metadata": {
        "id": "Po_ThN89rEWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ii) Sample from the decoder's predictions"
      ],
      "metadata": {
        "id": "4muFfacgrL9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking the decoder's logit outputs and samples token IDs from the distribution\n",
        "def sample(self, logits, temperature):\n",
        "  shape_checker = ShapeChecker()\n",
        "  # 't' is usually 1 here.\n",
        "  shape_checker(logits, ('batch', 't', 'vocab'))\n",
        "  shape_checker(self.token_mask, ('vocab',))\n",
        "\n",
        "  token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "  shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n",
        "\n",
        "  # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
        "  logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "\n",
        "  if temperature == 0.0:\n",
        "    new_tokens = tf.argmax(logits, axis=-1)\n",
        "  else: \n",
        "    logits = tf.squeeze(logits, axis=1)\n",
        "    new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                        num_samples=1)\n",
        "\n",
        "  shape_checker(new_tokens, ('batch', 't'))\n",
        "\n",
        "  return new_tokens"
      ],
      "metadata": {
        "id": "aYIHYKfJrKOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Translator.sample = sample"
      ],
      "metadata": {
        "id": "VteBiIP0rS8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random inputs (example)\n",
        "example_logits = tf.random.normal([5, 1, output_text_processor.vocabulary_size()])\n",
        "example_output_tokens = translator.sample(example_logits, temperature=1.0)\n",
        "example_output_tokens"
      ],
      "metadata": {
        "id": "jNDKC8OyrZ2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iii) Implement translation loop"
      ],
      "metadata": {
        "id": "j5Nr3PkorXYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taking the results into python lists before joining them  using tf.concat into tensors.\n",
        "# This unfolds the graph out to max_length iterations.\n",
        "def translate_unrolled(self,\n",
        "                       input_text, *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "  batch_size = tf.shape(input_text)[0]\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "  dec_state = enc_state\n",
        "  new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "  result_tokens = []\n",
        "  attention = []\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                             enc_output=enc_output,\n",
        "                             mask=(input_tokens!=0))\n",
        "\n",
        "    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "    attention.append(dec_result.attention_weights)\n",
        "\n",
        "    new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "    # If a sequence produces an `end_token`, set it `done`\n",
        "    done = done | (new_tokens == self.end_token)\n",
        "    # Once a sequence is done it only produces 0-padding.\n",
        "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "    # Collect the generated tokens\n",
        "    result_tokens.append(new_tokens)\n",
        "\n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Convert the list of generates token ids to a list of strings.\n",
        "  result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "  result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "  if return_attention:\n",
        "    attention_stack = tf.concat(attention, axis=1)\n",
        "    return {'text': result_text, 'attention': attention_stack}\n",
        "  else:\n",
        "    return {'text': result_text}"
      ],
      "metadata": {
        "id": "spIWGzjNrVsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Translator.translate = translate_unrolled"
      ],
      "metadata": {
        "id": "uSzYfqdFripi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running a simple input to view the translation\n",
        "%%time\n",
        "input_text = tf.constant([\n",
        "    'Boiboen che igesunotgei eng’ oret.', # \"Blessed are the undefiled in the way.\"\n",
        "    'a kiprutoiyo eng ngony ameungena ngatutiguk', # \"i am a stranger in the earth hide not thy commandments from me\"\n",
        "])\n",
        "\n",
        "\n",
        "result = translator.translate(\n",
        "    input_text = input_text)\n",
        "\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print()"
      ],
      "metadata": {
        "id": "Gncwm1c6rlmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S8O1d4UDnU4b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Conclusion"
      ],
      "metadata": {
        "id": "0tcB73funwpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a). Did we have the right data? The dataset is insufficient, to accurately train the model. A larger dataset with more characters needs in order to improve prediction accuracy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rj2G7roFn1hN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "b). Do we need other data to answer our question? Yes"
      ],
      "metadata": {
        "id": "MjuvLmo6bF01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Did we have the right question? Yes"
      ],
      "metadata": {
        "id": "rS0AjWVvbaPM"
      }
    }
  ]
}